{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900324</td>\n",
       "      <td>0.358590</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>0.126823</td>\n",
       "      <td>0.133306</td>\n",
       "      <td>0.119125</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794681</td>\n",
       "      <td>0.375387</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171923</td>\n",
       "      <td>0.283859</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>0.325912</td>\n",
       "      <td>0.345083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909029</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.423169</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.063032</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.077342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478893</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.081289</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>0.048774</td>\n",
       "      <td>0.054478</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.201360</td>\n",
       "      <td>0.099349</td>\n",
       "      <td>0.141336</td>\n",
       "      <td>0.120934</td>\n",
       "      <td>0.108516</td>\n",
       "      <td>0.096393</td>\n",
       "      <td>0.093436</td>\n",
       "      <td>0.100828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.000000  0.900324  0.358590  0.051459  0.046596  0.126823  0.133306   \n",
       "1  1.000000  0.794681  0.375387  0.116883  0.000000  0.171923  0.283859   \n",
       "2  0.909029  0.791482  0.423169  0.186712  0.000000  0.007836  0.063032   \n",
       "3  1.000000  0.478893  0.056760  0.064176  0.081289  0.072732  0.055619   \n",
       "4  1.000000  0.867238  0.201360  0.099349  0.141336  0.120934  0.108516   \n",
       "\n",
       "        7         8         9    ...  178  179  180  181  182  183  184  185  \\\n",
       "0  0.119125  0.110616  0.113047  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.293754  0.325912  0.345083  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.077002  0.074957  0.077342  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.048774  0.054478  0.041643  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.096393  0.093436  0.100828  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   186  187  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal = pd.read_csv(\"/Users/lorenzogonzalez/Downloads/LNLL Data Challenge/src/data/ptbdb_normal.csv\", header = None)\n",
    "df_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.932233</td>\n",
       "      <td>0.869679</td>\n",
       "      <td>0.886186</td>\n",
       "      <td>0.929626</td>\n",
       "      <td>0.908775</td>\n",
       "      <td>0.933970</td>\n",
       "      <td>0.801043</td>\n",
       "      <td>0.749783</td>\n",
       "      <td>0.687229</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606941</td>\n",
       "      <td>0.384181</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.223567</td>\n",
       "      <td>0.276836</td>\n",
       "      <td>0.253430</td>\n",
       "      <td>0.184826</td>\n",
       "      <td>0.153349</td>\n",
       "      <td>0.121872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.923963</td>\n",
       "      <td>0.853303</td>\n",
       "      <td>0.791859</td>\n",
       "      <td>0.734255</td>\n",
       "      <td>0.672043</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.670507</td>\n",
       "      <td>0.667435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.977819</td>\n",
       "      <td>0.899261</td>\n",
       "      <td>0.230129</td>\n",
       "      <td>0.032348</td>\n",
       "      <td>0.142329</td>\n",
       "      <td>0.223660</td>\n",
       "      <td>0.328096</td>\n",
       "      <td>0.367837</td>\n",
       "      <td>0.381701</td>\n",
       "      <td>0.389094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935618</td>\n",
       "      <td>0.801661</td>\n",
       "      <td>0.805815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722741</td>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.454829</td>\n",
       "      <td>0.319834</td>\n",
       "      <td>0.266874</td>\n",
       "      <td>0.308411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.932233  0.869679  0.886186  0.929626  0.908775  0.933970  0.801043   \n",
       "1  1.000000  0.606941  0.384181  0.254237  0.223567  0.276836  0.253430   \n",
       "2  1.000000  0.951613  0.923963  0.853303  0.791859  0.734255  0.672043   \n",
       "3  0.977819  0.899261  0.230129  0.032348  0.142329  0.223660  0.328096   \n",
       "4  0.935618  0.801661  0.805815  1.000000  0.722741  0.480789  0.454829   \n",
       "\n",
       "        7         8         9    ...  178  179  180  181  182  183  184  185  \\\n",
       "0  0.749783  0.687229  0.635100  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.184826  0.153349  0.121872  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.685100  0.670507  0.667435  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.367837  0.381701  0.389094  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.319834  0.266874  0.308411  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   186  187  \n",
       "0  0.0  1.0  \n",
       "1  0.0  1.0  \n",
       "2  0.0  1.0  \n",
       "3  0.0  1.0  \n",
       "4  0.0  1.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abnormal = pd.read_csv(\"/Users/lorenzogonzalez/Downloads/LNLL Data Challenge/src/data/ptbdb_abnormal.csv\", header = None)\n",
    "df_abnormal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4046, 188)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10506, 188)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abnormal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1488.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_col = len(df_normal.columns) - 1\n",
    "time = np.arange(0, num_col)/125\n",
    "time = time * 1000\n",
    "time[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>24.0</th>\n",
       "      <th>32.0</th>\n",
       "      <th>40.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>56.0</th>\n",
       "      <th>64.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1424.0</th>\n",
       "      <th>1432.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1448.0</th>\n",
       "      <th>1456.0</th>\n",
       "      <th>1464.0</th>\n",
       "      <th>1472.0</th>\n",
       "      <th>1480.0</th>\n",
       "      <th>1488.0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900324</td>\n",
       "      <td>0.358590</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>0.126823</td>\n",
       "      <td>0.133306</td>\n",
       "      <td>0.119125</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794681</td>\n",
       "      <td>0.375387</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171923</td>\n",
       "      <td>0.283859</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>0.325912</td>\n",
       "      <td>0.345083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909029</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.423169</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.063032</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.077342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478893</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.081289</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>0.048774</td>\n",
       "      <td>0.054478</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.201360</td>\n",
       "      <td>0.099349</td>\n",
       "      <td>0.141336</td>\n",
       "      <td>0.120934</td>\n",
       "      <td>0.108516</td>\n",
       "      <td>0.096393</td>\n",
       "      <td>0.093436</td>\n",
       "      <td>0.100828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0       8.0      16.0      24.0      32.0      40.0      48.0  \\\n",
       "0  1.000000  0.900324  0.358590  0.051459  0.046596  0.126823  0.133306   \n",
       "1  1.000000  0.794681  0.375387  0.116883  0.000000  0.171923  0.283859   \n",
       "2  0.909029  0.791482  0.423169  0.186712  0.000000  0.007836  0.063032   \n",
       "3  1.000000  0.478893  0.056760  0.064176  0.081289  0.072732  0.055619   \n",
       "4  1.000000  0.867238  0.201360  0.099349  0.141336  0.120934  0.108516   \n",
       "\n",
       "       56.0      64.0      72.0  ...  1424.0  1432.0  1440.0  1448.0  1456.0  \\\n",
       "0  0.119125  0.110616  0.113047  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.293754  0.325912  0.345083  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "2  0.077002  0.074957  0.077342  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3  0.048774  0.054478  0.041643  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "4  0.096393  0.093436  0.100828  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   1464.0  1472.0  1480.0  1488.0  label  \n",
       "0     0.0     0.0     0.0     0.0    0.0  \n",
       "1     0.0     0.0     0.0     0.0    0.0  \n",
       "2     0.0     0.0     0.0     0.0    0.0  \n",
       "3     0.0     0.0     0.0     0.0    0.0  \n",
       "4     0.0     0.0     0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal.columns = list(time) + [\"label\"]\n",
    "df_abnormal.columns = list(time) + [\"label\"]\n",
    "\n",
    "df_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>24.0</th>\n",
       "      <th>32.0</th>\n",
       "      <th>40.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>56.0</th>\n",
       "      <th>64.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1424.0</th>\n",
       "      <th>1432.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1448.0</th>\n",
       "      <th>1456.0</th>\n",
       "      <th>1464.0</th>\n",
       "      <th>1472.0</th>\n",
       "      <th>1480.0</th>\n",
       "      <th>1488.0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900324</td>\n",
       "      <td>0.358590</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>0.126823</td>\n",
       "      <td>0.133306</td>\n",
       "      <td>0.119125</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794681</td>\n",
       "      <td>0.375387</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171923</td>\n",
       "      <td>0.283859</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>0.325912</td>\n",
       "      <td>0.345083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909029</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.423169</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.063032</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.077342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478893</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.081289</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>0.048774</td>\n",
       "      <td>0.054478</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.201360</td>\n",
       "      <td>0.099349</td>\n",
       "      <td>0.141336</td>\n",
       "      <td>0.120934</td>\n",
       "      <td>0.108516</td>\n",
       "      <td>0.096393</td>\n",
       "      <td>0.093436</td>\n",
       "      <td>0.100828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0       8.0      16.0      24.0      32.0      40.0      48.0  \\\n",
       "0  1.000000  0.900324  0.358590  0.051459  0.046596  0.126823  0.133306   \n",
       "1  1.000000  0.794681  0.375387  0.116883  0.000000  0.171923  0.283859   \n",
       "2  0.909029  0.791482  0.423169  0.186712  0.000000  0.007836  0.063032   \n",
       "3  1.000000  0.478893  0.056760  0.064176  0.081289  0.072732  0.055619   \n",
       "4  1.000000  0.867238  0.201360  0.099349  0.141336  0.120934  0.108516   \n",
       "\n",
       "       56.0      64.0      72.0  ...  1424.0  1432.0  1440.0  1448.0  1456.0  \\\n",
       "0  0.119125  0.110616  0.113047  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.293754  0.325912  0.345083  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "2  0.077002  0.074957  0.077342  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3  0.048774  0.054478  0.041643  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "4  0.096393  0.093436  0.100828  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   1464.0  1472.0  1480.0  1488.0  label  \n",
       "0     0.0     0.0     0.0     0.0    0.0  \n",
       "1     0.0     0.0     0.0     0.0    0.0  \n",
       "2     0.0     0.0     0.0     0.0    0.0  \n",
       "3     0.0     0.0     0.0     0.0    0.0  \n",
       "4     0.0     0.0     0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_normal, df_abnormal], axis = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Abnormal    10506\n",
       "Normal       4046\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {0: \"Normal\", 1: \"Abnormal\"}\n",
    "counts = df[\"label\"].value_counts()\n",
    "counts.index = counts.index.map(classes)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.title(\"Abnormal Vs. Normal\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11641, 187), (1455, 187), (1456, 187), (1455,), (1456,), (11641,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenzogonzalez/Downloads/LNLL Data Challenge/venv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters = 64,kernel_size = 3, activation = \"relu\", input_shape = (x_train.shape[1],1)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(filters = 128,kernel_size = 3, activation = \"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(64, return_sequences = True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(16, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.7188 - loss: 0.5779 - val_accuracy: 0.7349 - val_loss: 0.4938\n",
      "Epoch 2/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7247 - loss: 0.5070 - val_accuracy: 0.7466 - val_loss: 0.4895\n",
      "Epoch 3/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.7548 - loss: 0.4835 - val_accuracy: 0.7878 - val_loss: 0.4374\n",
      "Epoch 4/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7974 - loss: 0.4163 - val_accuracy: 0.8235 - val_loss: 0.3669\n",
      "Epoch 5/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.8299 - loss: 0.3558 - val_accuracy: 0.8729 - val_loss: 0.2625\n",
      "Epoch 6/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.8719 - loss: 0.2966 - val_accuracy: 0.9045 - val_loss: 0.2292\n",
      "Epoch 7/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9023 - loss: 0.2444 - val_accuracy: 0.8860 - val_loss: 0.2887\n",
      "Epoch 8/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.8993 - loss: 0.2405 - val_accuracy: 0.9313 - val_loss: 0.1793\n",
      "Epoch 9/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9136 - loss: 0.2078 - val_accuracy: 0.9004 - val_loss: 0.2562\n",
      "Epoch 10/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9161 - loss: 0.2051 - val_accuracy: 0.9464 - val_loss: 0.1387\n",
      "Epoch 11/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9263 - loss: 0.1802 - val_accuracy: 0.9416 - val_loss: 0.1397\n",
      "Epoch 12/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9340 - loss: 0.1613 - val_accuracy: 0.9444 - val_loss: 0.1408\n",
      "Epoch 13/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9395 - loss: 0.1528 - val_accuracy: 0.9519 - val_loss: 0.1323\n",
      "Epoch 14/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9447 - loss: 0.1455 - val_accuracy: 0.9547 - val_loss: 0.1177\n",
      "Epoch 15/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.9585 - loss: 0.1173 - val_accuracy: 0.9574 - val_loss: 0.1199\n",
      "Epoch 16/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.9559 - loss: 0.1231 - val_accuracy: 0.9677 - val_loss: 0.0940\n",
      "Epoch 17/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9629 - loss: 0.0974 - val_accuracy: 0.9657 - val_loss: 0.0912\n",
      "Epoch 18/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9643 - loss: 0.1025 - val_accuracy: 0.9773 - val_loss: 0.0713\n",
      "Epoch 19/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9647 - loss: 0.0898 - val_accuracy: 0.9732 - val_loss: 0.0707\n",
      "Epoch 20/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9702 - loss: 0.0811 - val_accuracy: 0.9732 - val_loss: 0.0664\n",
      "Epoch 21/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.9667 - loss: 0.0915 - val_accuracy: 0.9780 - val_loss: 0.0660\n",
      "Epoch 22/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9696 - loss: 0.0868 - val_accuracy: 0.9828 - val_loss: 0.0488\n",
      "Epoch 23/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9744 - loss: 0.0679 - val_accuracy: 0.9766 - val_loss: 0.0641\n",
      "Epoch 24/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9772 - loss: 0.0665 - val_accuracy: 0.9828 - val_loss: 0.0392\n",
      "Epoch 25/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9724 - loss: 0.0730 - val_accuracy: 0.9808 - val_loss: 0.0578\n",
      "Epoch 26/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9691 - loss: 0.0872 - val_accuracy: 0.9787 - val_loss: 0.0622\n",
      "Epoch 27/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9796 - loss: 0.0609 - val_accuracy: 0.9801 - val_loss: 0.0526\n",
      "Epoch 28/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9804 - loss: 0.0638 - val_accuracy: 0.9718 - val_loss: 0.0683\n",
      "Epoch 29/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9781 - loss: 0.0615 - val_accuracy: 0.9828 - val_loss: 0.0525\n",
      "Epoch 30/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9770 - loss: 0.0609 - val_accuracy: 0.9801 - val_loss: 0.0500\n",
      "Epoch 31/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9775 - loss: 0.0626 - val_accuracy: 0.9718 - val_loss: 0.0735\n",
      "Epoch 32/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9771 - loss: 0.0643 - val_accuracy: 0.9849 - val_loss: 0.0444\n",
      "Epoch 33/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9865 - loss: 0.0401 - val_accuracy: 0.9821 - val_loss: 0.0492\n",
      "Epoch 34/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9845 - loss: 0.0447 - val_accuracy: 0.9705 - val_loss: 0.0831\n",
      "Epoch 35/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9845 - loss: 0.0508 - val_accuracy: 0.9808 - val_loss: 0.0450\n",
      "Epoch 36/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9870 - loss: 0.0404 - val_accuracy: 0.9842 - val_loss: 0.0525\n",
      "Epoch 37/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9856 - loss: 0.0516 - val_accuracy: 0.9787 - val_loss: 0.0546\n",
      "Epoch 38/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9847 - loss: 0.0404 - val_accuracy: 0.9870 - val_loss: 0.0355\n",
      "Epoch 39/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 108ms/step - accuracy: 0.9807 - loss: 0.0524 - val_accuracy: 0.9897 - val_loss: 0.0276\n",
      "Epoch 40/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9876 - loss: 0.0369 - val_accuracy: 0.9835 - val_loss: 0.0420\n",
      "Epoch 41/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9882 - loss: 0.0324 - val_accuracy: 0.9870 - val_loss: 0.0384\n",
      "Epoch 42/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9770 - loss: 0.0615 - val_accuracy: 0.9821 - val_loss: 0.0564\n",
      "Epoch 43/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9837 - loss: 0.0386 - val_accuracy: 0.9897 - val_loss: 0.0332\n",
      "Epoch 44/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9881 - loss: 0.0350 - val_accuracy: 0.9911 - val_loss: 0.0327\n",
      "Epoch 45/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9864 - loss: 0.0376 - val_accuracy: 0.9897 - val_loss: 0.0361\n",
      "Epoch 46/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9913 - loss: 0.0274 - val_accuracy: 0.9911 - val_loss: 0.0235\n",
      "Epoch 47/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9908 - loss: 0.0303 - val_accuracy: 0.9835 - val_loss: 0.0532\n",
      "Epoch 48/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9889 - loss: 0.0326 - val_accuracy: 0.9924 - val_loss: 0.0199\n",
      "Epoch 49/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9911 - loss: 0.0293 - val_accuracy: 0.9897 - val_loss: 0.0253\n",
      "Epoch 50/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9868 - loss: 0.0390 - val_accuracy: 0.9876 - val_loss: 0.0344\n",
      "Epoch 51/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9886 - loss: 0.0318 - val_accuracy: 0.9897 - val_loss: 0.0343\n",
      "Epoch 52/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9880 - loss: 0.0338 - val_accuracy: 0.9904 - val_loss: 0.0284\n",
      "Epoch 53/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0416 - val_accuracy: 0.9890 - val_loss: 0.0300\n",
      "Epoch 54/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9902 - loss: 0.0272 - val_accuracy: 0.9952 - val_loss: 0.0162\n",
      "Epoch 55/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9914 - loss: 0.0272 - val_accuracy: 0.9924 - val_loss: 0.0168\n",
      "Epoch 56/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9921 - loss: 0.0245 - val_accuracy: 0.9883 - val_loss: 0.0327\n",
      "Epoch 57/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9897 - loss: 0.0285 - val_accuracy: 0.9890 - val_loss: 0.0278\n",
      "Epoch 58/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9912 - loss: 0.0286 - val_accuracy: 0.9897 - val_loss: 0.0258\n",
      "Epoch 59/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9926 - loss: 0.0269 - val_accuracy: 0.9883 - val_loss: 0.0327\n",
      "Epoch 60/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9909 - loss: 0.0256 - val_accuracy: 0.9918 - val_loss: 0.0221\n",
      "Epoch 61/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9912 - loss: 0.0246 - val_accuracy: 0.9918 - val_loss: 0.0229\n",
      "Epoch 62/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9914 - loss: 0.0245 - val_accuracy: 0.9938 - val_loss: 0.0230\n",
      "Epoch 63/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9912 - loss: 0.0310 - val_accuracy: 0.9938 - val_loss: 0.0173\n",
      "Epoch 64/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9910 - loss: 0.0270 - val_accuracy: 0.9883 - val_loss: 0.0337\n",
      "Epoch 65/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9936 - loss: 0.0167 - val_accuracy: 0.9876 - val_loss: 0.0404\n",
      "Epoch 66/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9920 - loss: 0.0242 - val_accuracy: 0.9897 - val_loss: 0.0326\n",
      "Epoch 67/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 193ms/step - accuracy: 0.9885 - loss: 0.0354 - val_accuracy: 0.9883 - val_loss: 0.0313\n",
      "Epoch 68/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9909 - loss: 0.0281 - val_accuracy: 0.9876 - val_loss: 0.0342\n",
      "Epoch 69/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9935 - loss: 0.0177 - val_accuracy: 0.9904 - val_loss: 0.0321\n",
      "Epoch 70/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9887 - loss: 0.0324 - val_accuracy: 0.9952 - val_loss: 0.0134\n",
      "Epoch 71/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9952 - loss: 0.0170 - val_accuracy: 0.9938 - val_loss: 0.0194\n",
      "Epoch 72/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9907 - loss: 0.0236 - val_accuracy: 0.9931 - val_loss: 0.0235\n",
      "Epoch 73/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9919 - loss: 0.0246 - val_accuracy: 0.9918 - val_loss: 0.0293\n",
      "Epoch 74/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9937 - loss: 0.0203 - val_accuracy: 0.9918 - val_loss: 0.0314\n",
      "Epoch 75/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9918 - loss: 0.0260 - val_accuracy: 0.9924 - val_loss: 0.0264\n",
      "Epoch 76/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9935 - loss: 0.0199 - val_accuracy: 0.9959 - val_loss: 0.0124\n",
      "Epoch 77/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9899 - loss: 0.0278 - val_accuracy: 0.9918 - val_loss: 0.0219\n",
      "Epoch 78/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9951 - loss: 0.0135 - val_accuracy: 0.9938 - val_loss: 0.0241\n",
      "Epoch 79/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9924 - loss: 0.0213 - val_accuracy: 0.9924 - val_loss: 0.0249\n",
      "Epoch 80/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9958 - loss: 0.0150 - val_accuracy: 0.9773 - val_loss: 0.0761\n",
      "Epoch 81/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9855 - loss: 0.0480 - val_accuracy: 0.9931 - val_loss: 0.0258\n",
      "Epoch 82/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9916 - loss: 0.0227 - val_accuracy: 0.9918 - val_loss: 0.0263\n",
      "Epoch 83/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9939 - loss: 0.0153 - val_accuracy: 0.9924 - val_loss: 0.0245\n",
      "Epoch 84/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 30ms/step - accuracy: 0.9945 - loss: 0.0166 - val_accuracy: 0.9787 - val_loss: 0.0716\n",
      "Epoch 85/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9893 - loss: 0.0306 - val_accuracy: 0.9938 - val_loss: 0.0236\n",
      "Epoch 86/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9948 - loss: 0.0164 - val_accuracy: 0.9952 - val_loss: 0.0178\n",
      "Epoch 87/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9945 - loss: 0.0177 - val_accuracy: 0.9924 - val_loss: 0.0266\n",
      "Epoch 88/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9938 - loss: 0.0182 - val_accuracy: 0.9897 - val_loss: 0.0330\n",
      "Epoch 89/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9950 - loss: 0.0164 - val_accuracy: 0.9945 - val_loss: 0.0233\n",
      "Epoch 90/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9949 - loss: 0.0164 - val_accuracy: 0.9959 - val_loss: 0.0188\n",
      "Epoch 91/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9930 - loss: 0.0162 - val_accuracy: 0.9938 - val_loss: 0.0144\n",
      "Epoch 92/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9924 - loss: 0.0202 - val_accuracy: 0.9924 - val_loss: 0.0203\n",
      "Epoch 93/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9960 - loss: 0.0110 - val_accuracy: 0.9890 - val_loss: 0.0363\n",
      "Epoch 94/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - accuracy: 0.9893 - loss: 0.0281 - val_accuracy: 0.9938 - val_loss: 0.0153\n",
      "Epoch 95/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.9919 - loss: 0.0243 - val_accuracy: 0.9938 - val_loss: 0.0257\n",
      "Epoch 96/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9945 - loss: 0.0155 - val_accuracy: 0.9904 - val_loss: 0.0340\n",
      "Epoch 97/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.9924 - loss: 0.0177 - val_accuracy: 0.9904 - val_loss: 0.0269\n",
      "Epoch 98/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9950 - loss: 0.0156 - val_accuracy: 0.9931 - val_loss: 0.0259\n",
      "Epoch 99/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9963 - loss: 0.0122 - val_accuracy: 0.9931 - val_loss: 0.0248\n",
      "Epoch 100/100\n",
      "\u001b[1m364/364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.9961 - loss: 0.0092 - val_accuracy: 0.9876 - val_loss: 0.0347\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, batch_size = 32, validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "870401a200a29d07963db08632116b8200d46cae1ca58fc9ed95faf0d3a7e4ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
